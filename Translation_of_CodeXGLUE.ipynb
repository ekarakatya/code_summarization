{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgwxKYpM8_fG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f163e31-0cad-4f70-dd4b-c83c3d605242"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 24 23:32:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# copy author's repo to google disk: https://github.com/wasiahmad/PLBARThttps://github.com/wasiahmad/PLBART "
      ],
      "metadata": {
        "id": "VGjxhI_U9cQn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4078728a-32d1-45cc-ab83-3e0c59855650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate English docstrings to Russian"
      ],
      "metadata": {
        "id": "vq8vfYdn06Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data (train 250k)\n",
        "%cd /content/drive/MyDrive/PLBART-main/data/codeXglue/\n",
        "!bash /content/drive/MyDrive/PLBART-main/data/codeXglue/download.sh"
      ],
      "metadata": {
        "id": "bY4ORhSjkrkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "yjb_BOLE3ycS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "7zgfDgZ23ul0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test data"
      ],
      "metadata": {
        "id": "p23SvOv6VeyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_codes = []\n",
        "test_docstrings = []\n",
        "with open('/content/drive/MyDrive/PLBART-main/data/codeXglue/code-to-text/python/test.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    example = json.loads(line)\n",
        "    test_codes.append(example['code'])\n",
        "    comment = ' '.join(example['docstring_tokens'])\n",
        "    test_docstrings.append(comment)"
      ],
      "metadata": {
        "id": "O9S1v21k7GyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_trans = translator.translate(test_docstrings, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "x_5OpQKQ1SxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_test_ru', 'w', encoding='utf-8') as f:\n",
        "  for y in test_trans:\n",
        "    f.write(y.text + '\\n') "
      ],
      "metadata": {
        "id": "hFeL8ukS1Szs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('test.jsonl', 'w', encoding='utf-8') as f:\n",
        "  for x, y in zip(test_codes, test_trans):\n",
        "    result = dict()\n",
        "    result['code'] = x\n",
        "    result['docstring_tokens'] = my_split(y.text)\n",
        "    f.write(json.dumps(result)+'\\n')"
      ],
      "metadata": {
        "id": "7C--wcg01S48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "with open('test.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    if s == 10:\n",
        "      break\n",
        "    example = json.loads(line)\n",
        "    print(example['code'][:10])\n",
        "    comment = ' '.join(example['docstring_tokens'])\n",
        "    print(comment)\n",
        "    print('---------------------')\n",
        "    s += 1"
      ],
      "metadata": {
        "id": "Wb4gA47e1S7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc45e10d-71c8-44c7-eb9b-39ca5110578c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def sina_x\n",
            "str - > list Преобразование XML в список URL . Из Билиграба .\n",
            "---------------------\n",
            "def dailym\n",
            "Загружает видео Dailymotion по URL .\n",
            "---------------------\n",
            "def sina_d\n",
            "Загружает видео Sina по URL .\n",
            "---------------------\n",
            "def sprint\n",
            "Форматировать текст с цветом или другими эффектами в экранированную строку ANSI .\n",
            "---------------------\n",
            "def print_\n",
            "Распечатайте сообщение журнала для стандартной ошибки .\n",
            "---------------------\n",
            "def e(mess\n",
            "Распечатайте сообщение журнала ошибок .\n",
            "---------------------\n",
            "def wtf(me\n",
            "Какая ужасная неудача !\n",
            "---------------------\n",
            "def detect\n",
            "Обнаружить операционную систему .\n",
            "---------------------\n",
            "def vimeo_\n",
            "ул - > Нет\n",
            "---------------------\n",
            "def ckplay\n",
            "str - > dict Информация для контента CKPlayer API .\n",
            "---------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/trans_test_ru /content/drive/MyDrive/PLBART-main/additional_data"
      ],
      "metadata": {
        "id": "rRtOaCYj1S-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/test.jsonl /content/drive/MyDrive/PLBART-main/additional_data"
      ],
      "metadata": {
        "id": "Ws-zImQ51TB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "with open('/content/test.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    if s == 10:\n",
        "      break\n",
        "    example = json.loads(line)\n",
        "    print(example)\n",
        "    s += 1"
      ],
      "metadata": {
        "id": "WdU3fzAw1TEX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de76224-60e7-46f0-fd27-aa6e53a2b7f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'code': 'def sina_xml_to_url_list(xml_data):\\n    \"\"\"str->list\\n    Convert XML to URL List.\\n    From Biligrab.\\n    \"\"\"\\n    rawurl = []\\n    dom = parseString(xml_data)\\n    for node in dom.getElementsByTagName(\\'durl\\'):\\n        url = node.getElementsByTagName(\\'url\\')[0]\\n        rawurl.append(url.childNodes[0].data)\\n    return rawurl', 'docstring_tokens': ['str', '-', '>', 'list', 'Преобразование', 'XML', 'в', 'список', 'URL', '.', 'Из', 'Билиграба', '.']}\n",
            "{'code': 'def dailymotion_download(url, output_dir=\\'.\\', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Dailymotion videos by URL.\\n    \"\"\"\\n\\n    html = get_content(rebuilt_url(url))\\n    info = json.loads(match1(html, r\\'qualities\":({.+?}),\"\\'))\\n    title = match1(html, r\\'\"video_title\"\\\\s*:\\\\s*\"([^\"]+)\"\\') or \\\\\\n            match1(html, r\\'\"title\"\\\\s*:\\\\s*\"([^\"]+)\"\\')\\n    title = unicodize(title)\\n\\n    for quality in [\\'1080\\',\\'720\\',\\'480\\',\\'380\\',\\'240\\',\\'144\\',\\'auto\\']:\\n        try:\\n            real_url = info[quality][1][\"url\"]\\n            if real_url:\\n                break\\n        except KeyError:\\n            pass\\n\\n    mime, ext, size = url_info(real_url)\\n\\n    print_info(site_info, title, mime, size)\\n    if not info_only:\\n        download_urls([real_url], title, ext, size, output_dir=output_dir, merge=merge)', 'docstring_tokens': ['Загружает', 'видео', 'Dailymotion', 'по', 'URL', '.']}\n",
            "{'code': 'def sina_download(url, output_dir=\\'.\\', merge=True, info_only=False, **kwargs):\\n    \"\"\"Downloads Sina videos by URL.\\n    \"\"\"\\n    if \\'news.sina.com.cn/zxt\\' in url:\\n        sina_zxt(url, output_dir=output_dir, merge=merge, info_only=info_only, **kwargs)\\n        return\\n\\n    vid = match1(url, r\\'vid=(\\\\d+)\\')\\n    if vid is None:\\n        video_page = get_content(url)\\n        vid = hd_vid = match1(video_page, r\\'hd_vid\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\')\\n        if hd_vid == \\'0\\':\\n            vids = match1(video_page, r\\'[^\\\\w]vid\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\').split(\\'|\\')\\n            vid = vids[-1]\\n\\n    if vid is None:\\n        vid = match1(video_page, r\\'vid:\"?(\\\\d+)\"?\\')\\n    if vid:\\n        #title = match1(video_page, r\\'title\\\\s*:\\\\s*\\\\\\'([^\\\\\\']+)\\\\\\'\\')\\n        sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\\n    else:\\n        vkey = match1(video_page, r\\'vkey\\\\s*:\\\\s*\"([^\"]+)\"\\')\\n        if vkey is None:\\n            vid = match1(url, r\\'#(\\\\d+)\\')\\n            sina_download_by_vid(vid, output_dir=output_dir, merge=merge, info_only=info_only)\\n            return\\n        title = match1(video_page, r\\'title\\\\s*:\\\\s*\"([^\"]+)\"\\')\\n        sina_download_by_vkey(vkey, title=title, output_dir=output_dir, merge=merge, info_only=info_only)', 'docstring_tokens': ['Загружает', 'видео', 'Sina', 'по', 'URL', '.']}\n",
            "{'code': 'def sprint(text, *colors):\\n    \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\"\\n    return \"\\\\33[{}m{content}\\\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text', 'docstring_tokens': ['Форматировать', 'текст', 'с', 'цветом', 'или', 'другими', 'эффектами', 'в', 'экранированную', 'строку', 'ANSI', '.']}\n",
            "{'code': 'def print_log(text, *colors):\\n    \"\"\"Print a log message to standard error.\"\"\"\\n    sys.stderr.write(sprint(\"{}: {}\".format(script_name, text), *colors) + \"\\\\n\")', 'docstring_tokens': ['Распечатайте', 'сообщение', 'журнала', 'для', 'стандартной', 'ошибки', '.']}\n",
            "{'code': 'def e(message, exit_code=None):\\n    \"\"\"Print an error log message.\"\"\"\\n    print_log(message, YELLOW, BOLD)\\n    if exit_code is not None:\\n        sys.exit(exit_code)', 'docstring_tokens': ['Распечатайте', 'сообщение', 'журнала', 'ошибок', '.']}\n",
            "{'code': 'def wtf(message, exit_code=1):\\n    \"\"\"What a Terrible Failure!\"\"\"\\n    print_log(message, RED, BOLD)\\n    if exit_code is not None:\\n        sys.exit(exit_code)', 'docstring_tokens': ['Какая', 'ужасная', 'неудача', '!']}\n",
            "{'code': 'def detect_os():\\n    \"\"\"Detect operating system.\\n    \"\"\"\\n\\n    # Inspired by:\\n    # https://github.com/scivision/pybashutils/blob/78b7f2b339cb03b1c37df94015098bbe462f8526/pybashutils/windows_linux_detect.py\\n\\n    syst = system().lower()\\n    os = \\'unknown\\'\\n\\n    if \\'cygwin\\' in syst:\\n        os = \\'cygwin\\'\\n    elif \\'darwin\\' in syst:\\n        os = \\'mac\\'\\n    elif \\'linux\\' in syst:\\n        os = \\'linux\\'\\n        # detect WSL https://github.com/Microsoft/BashOnWindows/issues/423\\n        try:\\n            with open(\\'/proc/version\\', \\'r\\') as f:\\n                if \\'microsoft\\' in f.read().lower():\\n                    os = \\'wsl\\'\\n        except: pass\\n    elif \\'windows\\' in syst:\\n        os = \\'windows\\'\\n    elif \\'bsd\\' in syst:\\n        os = \\'bsd\\'\\n\\n    return os', 'docstring_tokens': ['Обнаружить', 'операционную', 'систему', '.']}\n",
            "{'code': 'def vimeo_download_by_channel(url, output_dir=\\'.\\', merge=False, info_only=False, **kwargs):\\n    \"\"\"str->None\"\"\"\\n    # https://vimeo.com/channels/464686\\n    channel_id = match1(url, r\\'http://vimeo.com/channels/(\\\\w+)\\')\\n    vimeo_download_by_channel_id(channel_id, output_dir, merge, info_only, **kwargs)', 'docstring_tokens': ['ул', '-', '>', 'Нет']}\n",
            "{'code': 'def ckplayer_get_info_by_xml(ckinfo):\\n    \"\"\"str->dict\\n    Information for CKPlayer API content.\"\"\"\\n    e = ET.XML(ckinfo)\\n    video_dict = {\\'title\\': \\'\\',\\n                  #\\'duration\\': 0,\\n                  \\'links\\': [],\\n                  \\'size\\': 0,\\n                  \\'flashvars\\': \\'\\',}\\n    dictified = dictify(e)[\\'ckplayer\\']\\n    if \\'info\\' in dictified:\\n        if \\'_text\\' in dictified[\\'info\\'][0][\\'title\\'][0]:  #title\\n            video_dict[\\'title\\'] = dictified[\\'info\\'][0][\\'title\\'][0][\\'_text\\'].strip()\\n\\n    #if dictify(e)[\\'ckplayer\\'][\\'info\\'][0][\\'title\\'][0][\\'_text\\'].strip():  #duration\\n        #video_dict[\\'title\\'] = dictify(e)[\\'ckplayer\\'][\\'info\\'][0][\\'title\\'][0][\\'_text\\'].strip()\\n\\n    if \\'_text\\' in dictified[\\'video\\'][0][\\'size\\'][0]:  #size exists for 1 piece\\n        video_dict[\\'size\\'] = sum([int(i[\\'size\\'][0][\\'_text\\']) for i in dictified[\\'video\\']])\\n\\n    if \\'_text\\' in dictified[\\'video\\'][0][\\'file\\'][0]:  #link exist\\n        video_dict[\\'links\\'] = [i[\\'file\\'][0][\\'_text\\'].strip() for i in dictified[\\'video\\']]\\n\\n    if \\'_text\\' in dictified[\\'flashvars\\'][0]:\\n        video_dict[\\'flashvars\\'] = dictified[\\'flashvars\\'][0][\\'_text\\'].strip()\\n\\n    return video_dict', 'docstring_tokens': ['str', '-', '>', 'dict', 'Информация', 'для', 'контента', 'CKPlayer', 'API', '.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translate train docstrings to Russian"
      ],
      "metadata": {
        "id": "3pEbeZ4PSaxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "metadata": {
        "id": "VZGt-o3VShlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from googletrans import Translator\n",
        "translator = Translator()"
      ],
      "metadata": {
        "id": "PQx7qILDShoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I neeed only train\n",
        "%rm /content/drive/MyDrive/PLBART-main/data/codeXglue/code-to-text/python/test.jsonl\n",
        "%rm /content/drive/MyDrive/PLBART-main/data/codeXglue/code-to-text/python/valid.jsonl"
      ],
      "metadata": {
        "id": "QUOaGvN5Sh1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "codes = []\n",
        "docstrings = []\n",
        "with open('/content/drive/MyDrive/PLBART-main/data/codeXglue/code-to-text/python/train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    example = json.loads(line)\n",
        "    codes.append(example['code'])\n",
        "    comment = ' '.join(example['docstring_tokens'])\n",
        "    docstrings.append(comment)"
      ],
      "metadata": {
        "id": "ddgAJGbsUb6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docstrings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA1VzDRQVuPE",
        "outputId": "6eecbbab-bd13-472d-d616-e40a54f4cb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(codes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsgA_4LkAd6s",
        "outputId": "7cdd2d41-68ce-4b78-f04c-ce4bc2c3c6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "251820"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_50 = docstrings[:50000]\n",
        "doc_100 = docstrings[50000:100000]\n",
        "doc_150 = docstrings[100000:150000]\n",
        "doc_200 = docstrings[150000:200000]\n",
        "doc_250 = docstrings[200000:]\n",
        "#Because translator is slow"
      ],
      "metadata": {
        "id": "MiFAuoqiV3Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_50 = translator.translate(doc_50, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "1VDHkCluUb_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_100 = translator.translate(doc_100, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "31WDESvtBlbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_150 = translator.translate(doc_150, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "LJ29PqE4Ahnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_200 = translator.translate(doc_200, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "ZXRw6Hvsafkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_250 = translator.translate(doc_250, src='en', dest='ru')"
      ],
      "metadata": {
        "id": "Yp5EmfOq1uGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0B22SOtY_gc6",
        "outputId": "15dbf147-f515-4e4c-8972-fbd24be2eb27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_train_50_ru.txt', 'w', encoding='utf-8') as f:\n",
        "  for y in trans_50:\n",
        "    f.write(y.text + '\\n') "
      ],
      "metadata": {
        "id": "oYNKPQVVUcBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_train_100_ru.txt', 'w', encoding='utf-8') as f:\n",
        "  for y in trans_100:\n",
        "    f.write(y.text + '\\n')"
      ],
      "metadata": {
        "id": "BCUXKTGx_MSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_train_150_ru', 'w', encoding='utf-8') as f:\n",
        "  for y in trans_150:\n",
        "    f.write(y.text + '\\n')"
      ],
      "metadata": {
        "id": "5lwisaZSBWdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_train_200_ru', 'w', encoding='utf-8') as f:\n",
        "  for y in trans_200:\n",
        "    f.write(y.text + '\\n')"
      ],
      "metadata": {
        "id": "wxse1vXpd6e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('trans_train_250_ru', 'w', encoding='utf-8') as f:\n",
        "  for y in trans_250:\n",
        "    f.write(y.text + '\\n')"
      ],
      "metadata": {
        "id": "0s2edofGQP6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/trans_train_250_ru /content/drive/MyDrive/PLBART-main/additional_data/"
      ],
      "metadata": {
        "id": "d5kXQezWBwD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string \n",
        "\n",
        "def my_split(x):\n",
        "  temp = x.split()\n",
        "  result = []\n",
        "  for tok in temp:\n",
        "    accum = ''\n",
        "    for t in tok: \n",
        "      if t not in string.punctuation:\n",
        "        accum += t\n",
        "      else:\n",
        "        if len(accum) > 0:\n",
        "          result.append(accum)\n",
        "        accum = ''\n",
        "        result.append(t)\n",
        "    if len(accum) > 0:\n",
        "      result.append(accum)\n",
        "  return result"
      ],
      "metadata": {
        "id": "asXD8wY2Wpi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trans_50 = []\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_50_ru', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_50.append(line.strip())"
      ],
      "metadata": {
        "id": "Zzz3MCIBWf-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trans_50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOO3flk_a2ku",
        "outputId": "3010b070-2bde-460b-e342-9c6f9c18f0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_50[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBraTWypa40E",
        "outputId": "4ffc789d-7c6c-4b9a-cdad-98bd634e9a46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Возвращает либо полную, либо усеченную версию строки таксономии в формате QIIME.',\n",
              " 'Убедитесь, что указанный путь к каталогу не существует, если создайте его. Метод перехватывает исключения OSError и возвращает описательное сообщение вместо повторного вызова ошибки.',\n",
              " 'Принимает либо путь к файлу, либо дескриптор открытого файла, проверяет действительность и возвращает дескриптор открытого файла или вызывает соответствующее исключение.',\n",
              " 'Найдите указанные пользователем категории на карте и создайте словарь, содержащий соответствующие данные для каждого типа в категориях. Типы нескольких категорий будут объединены таким образом, что каждая возможная комбинация будет иметь свою собственную запись в словаре.',\n",
              " 'Разбирает файл результатов unifrac в словарь',\n",
              " 'Функция для анализа данных из старой версии файла unifrac, полученного из Qiime версии 1. 8 и ранее.',\n",
              " 'Функция для анализа данных из более новой версии файла unifrac, полученного из Qiime версии 1. 9 и позже.',\n",
              " 'Определите сопоставление цветовой категории. Если был указан color_column, сопоставьте имена категорий со значениями цвета. В противном случае используйте цвета палитры для автоматического создания набора цветов для групповых значений.',\n",
              " 'вернуть обратное дополнение чтения',\n",
              " 'случайно перетасовать геном']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_100 = []\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_100_ru.txt', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_100.append(line.strip())"
      ],
      "metadata": {
        "id": "wrg9EEmIW1gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trans_100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBjwR36Na8lb",
        "outputId": "e0ef0022-cddd-4dca-959f-67b0dc8acf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_100[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuaOSWaWa_Q9",
        "outputId": "317479d0-6d81-414c-9086-20b235435068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Функция перевода по умолчанию для систем на основе Ubuntu',\n",
              " 'Функция перевода по умолчанию для openSUSE SLES и других систем на базе SUSE.',\n",
              " 'Возвращает соответствующее имя пакета для модуля Python.',\n",
              " 'Возвращает соответствующее имя основной ветки OpenStack для модуля Python.',\n",
              " 'для разрешения имен из командной строки',\n",
              " 'Получает значения разбиения по страницам, переданные через params строки запроса.',\n",
              " 'Создайте словарь call_types, описывающий, какие аргументы передавать f.',\n",
              " 'Создайте словарь аннотаций из комментариев типа Python2',\n",
              " 'Иногда предварительная обработка представления должна выполняться до того, как атрибут объекта будет установлен для представления. В этом случае просто верните объект, если он уже был установлен, когда он вызывается в будущем, поскольку нет необходимости делать еще один запрос.',\n",
              " 'Преобразует шестнадцатеричную временную метку в объект даты и времени.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
        "  for x, y in zip(codes[:100000], trans_50 + trans_100):\n",
        "    result = dict()\n",
        "    result['code'] = x\n",
        "    result['docstring_tokens'] = my_split(y)\n",
        "    f.write(json.dumps(result)+'\\n')\n",
        "\n",
        "  for x, y in zip(codes[100000:], trans_150 + trans_200 + trans_250):\n",
        "    result = dict()\n",
        "    result['code'] = x\n",
        "    result['docstring_tokens'] = my_split(y.text)\n",
        "    f.write(json.dumps(result)+'\\n')"
      ],
      "metadata": {
        "id": "aEa8S2FOWpna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "s = 0\n",
        "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    if s == 10:\n",
        "      break\n",
        "    example = json.loads(line)\n",
        "    print(example['code'][:10])\n",
        "    comment = ' '.join(example['docstring_tokens'])\n",
        "    print(comment)\n",
        "    print('---------------------')\n",
        "    s += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP7NVYtlWpqL",
        "outputId": "3c734578-4f08-4c85-f1fc-a81242bf3166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def split_\n",
            "Return either the full or truncated version of a QIIME - formatted taxonomy string .\n",
            "---------------------\n",
            "def ensure\n",
            "Check to make sure the supplied directory path does not exist if so create it . The method catches OSError exceptions and returns a descriptive message instead of re - raising the error .\n",
            "---------------------\n",
            "def file_h\n",
            "Takes either a file path or an open file handle checks validity and returns an open file handle or raises an appropriate Exception .\n",
            "---------------------\n",
            "def gather\n",
            "Find the user specified categories in the map and create a dictionary to contain the relevant data for each type within the categories . Multiple categories will have their types combined such that each possible combination will have its own entry in the dictionary .\n",
            "---------------------\n",
            "def parse_\n",
            "Parses the unifrac results file into a dictionary\n",
            "---------------------\n",
            "def parse_\n",
            "Function to parse data from older version of unifrac file obtained from Qiime version 1 . 8 and earlier .\n",
            "---------------------\n",
            "def parse_\n",
            "Function to parse data from newer version of unifrac file obtained from Qiime version 1 . 9 and later .\n",
            "---------------------\n",
            "def color_\n",
            "Determine color - category mapping . If color _ column was specified then map the category names to color values . Otherwise use the palettable colors to automatically generate a set of colors for the group values .\n",
            "---------------------\n",
            "def rev_c(\n",
            "return reverse completment of read\n",
            "---------------------\n",
            "def shuffl\n",
            "randomly shuffle genome\n",
            "---------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mv /content/trans_train_50_ru /content/drive/MyDrive/PLBART-main/additional_data"
      ],
      "metadata": {
        "id": "Lhj75O_1WpsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir /content/drive/MyDrive/PLBART-main/additional_data/trans_train_250/"
      ],
      "metadata": {
        "id": "QgdDOd0wrpNu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/train.jsonl /content/drive/MyDrive/PLBART-main/additional_data/trans_train_250/"
      ],
      "metadata": {
        "id": "qwe0R-KD_wik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "s = 0\n",
        "with open('/content/train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    if s == 10:\n",
        "      break\n",
        "    example = json.loads(line)\n",
        "    print(example)\n",
        "    s += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A4JHG_IWpvp",
        "outputId": "035e1f24-590f-4c8e-8c2d-127a8bb8319f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'code': 'def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \"\"\"\\n    level = level+\"__\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\";\")[0]', 'code_tokens': ['def', 'split_phylogeny', '(', 'p', ',', 'level', '=', '\"s\"', ')', ':', 'level', '=', 'level', '+', '\"__\"', 'result', '=', 'p', '.', 'split', '(', 'level', ')', 'return', 'result', '[', '0', ']', '+', 'level', '+', 'result', '[', '1', ']', '.', 'split', '(', '\";\"', ')', '[', '0', ']'], 'docstring_tokens': ['Возвращает', 'либо', 'полную', ',', 'либо', 'усеченную', 'версию', 'строки', 'таксономии', 'в', 'формате', 'QIIME', '.']}\n",
            "{'code': 'def ensure_dir(d):\\n    \"\"\"\\n    Check to make sure the supplied directory path does not exist, if so, create it. The\\n    method catches OSError exceptions and returns a descriptive message instead of\\n    re-raising the error.\\n\\n    :type d: str\\n    :param d: It is the full path to a directory.\\n\\n    :return: Does not return anything, but creates a directory path if it doesn\\'t exist\\n             already.\\n    \"\"\"\\n    if not os.path.exists(d):\\n        try:\\n            os.makedirs(d)\\n        except OSError as oe:\\n            # should not happen with os.makedirs\\n            # ENOENT: No such file or directory\\n            if os.errno == errno.ENOENT:\\n                msg = twdd(\"\"\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\"\"\")\\n                return msg.format(d)\\n            else:\\n                msg = twdd(\"\"\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\"\"\")\\n                return msg.format(d, oe.strerror)', 'code_tokens': ['def', 'ensure_dir', '(', 'd', ')', ':', 'if', 'not', 'os', '.', 'path', '.', 'exists', '(', 'd', ')', ':', 'try', ':', 'os', '.', 'makedirs', '(', 'd', ')', 'except', 'OSError', 'as', 'oe', ':', '# should not happen with os.makedirs', '# ENOENT: No such file or directory', 'if', 'os', '.', 'errno', '==', 'errno', '.', 'ENOENT', ':', 'msg', '=', 'twdd', '(', '\"\"\"One or more directories in the path ({}) do not exist. If\\n                           you are specifying a new directory for output, please ensure\\n                           all other directories in the path currently exist.\"\"\"', ')', 'return', 'msg', '.', 'format', '(', 'd', ')', 'else', ':', 'msg', '=', 'twdd', '(', '\"\"\"An error occurred trying to create the output directory\\n                           ({}) with message: {}\"\"\"', ')', 'return', 'msg', '.', 'format', '(', 'd', ',', 'oe', '.', 'strerror', ')'], 'docstring_tokens': ['Убедитесь', ',', 'что', 'указанный', 'путь', 'к', 'каталогу', 'не', 'существует', ',', 'если', 'создайте', 'его', '.', 'Метод', 'перехватывает', 'исключения', 'OSError', 'и', 'возвращает', 'описательное', 'сообщение', 'вместо', 'повторного', 'вызова', 'ошибки', '.']}\n",
            "{'code': 'def file_handle(fnh, mode=\"rU\"):\\n    \"\"\"\\n    Takes either a file path or an open file handle, checks validity and returns an open\\n    file handle or raises an appropriate Exception.\\n\\n    :type fnh: str\\n    :param fnh: It is the full path to a file, or open file handle\\n\\n    :type mode: str\\n    :param mode: The way in which this file will be used, for example to read or write or\\n                 both. By default, file will be opened in rU mode.\\n\\n    :return: Returns an opened file for appropriate usage.\\n    \"\"\"\\n    handle = None\\n    if isinstance(fnh, file):\\n        if fnh.closed:\\n            raise ValueError(\"Input file is closed.\")\\n        handle = fnh\\n    elif isinstance(fnh, str):\\n        handle = open(fnh, mode)\\n\\n    return handle', 'code_tokens': ['def', 'file_handle', '(', 'fnh', ',', 'mode', '=', '\"rU\"', ')', ':', 'handle', '=', 'None', 'if', 'isinstance', '(', 'fnh', ',', 'file', ')', ':', 'if', 'fnh', '.', 'closed', ':', 'raise', 'ValueError', '(', '\"Input file is closed.\"', ')', 'handle', '=', 'fnh', 'elif', 'isinstance', '(', 'fnh', ',', 'str', ')', ':', 'handle', '=', 'open', '(', 'fnh', ',', 'mode', ')', 'return', 'handle'], 'docstring_tokens': ['Принимает', 'либо', 'путь', 'к', 'файлу', ',', 'либо', 'дескриптор', 'открытого', 'файла', ',', 'проверяет', 'действительность', 'и', 'возвращает', 'дескриптор', 'открытого', 'файла', 'или', 'вызывает', 'соответствующее', 'исключение', '.']}\n",
            "{'code': 'def gather_categories(imap, header, categories=None):\\n    \"\"\"\\n    Find the user specified categories in the map and create a dictionary to contain the\\n    relevant data for each type within the categories. Multiple categories will have their\\n    types combined such that each possible combination will have its own entry in the\\n    dictionary.\\n\\n    :type imap: dict\\n    :param imap: The input mapping file data keyed by SampleID\\n    :type header: list\\n    :param header: The header line from the input mapping file. This will be searched for\\n                   the user-specified categories\\n    :type categories: list\\n    :param categories: The list of user-specified category column name from mapping file\\n    :rtype: dict\\n    :return: A sorted dictionary keyed on the combinations of all the types found within\\n             the user-specified categories. Each entry will contain an empty DataCategory\\n             namedtuple. If no categories are specified, a single entry with the key\\n             \\'default\\' will be returned\\n    \"\"\"\\n    # If no categories provided, return all SampleIDs\\n    if categories is None:\\n        return {\"default\": DataCategory(set(imap.keys()), {})}\\n\\n    cat_ids = [header.index(cat)\\n               for cat in categories if cat in header and \"=\" not in cat]\\n\\n    table = OrderedDict()\\n    conditions = defaultdict(set)\\n    for i, cat in enumerate(categories):\\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\\n            cat_name = header[header.index(cat.split(\"=\")[0])]\\n            conditions[cat_name].add(cat.split(\"=\")[1])\\n\\n    # If invalid categories or conditions identified, return all SampleIDs\\n    if not cat_ids and not conditions:\\n        return {\"default\": DataCategory(set(imap.keys()), {})}\\n\\n    #If only category column given, return column-wise SampleIDs\\n    if cat_ids and not conditions:\\n        for sid, row in imap.items():\\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\\n            if cat_name not in table:\\n                table[cat_name] = DataCategory(set(), {})\\n            table[cat_name].sids.add(sid)\\n        return table\\n\\n    # Collect all condition names\\n    cond_ids = set()\\n    for k in conditions:\\n        try:\\n            cond_ids.add(header.index(k))\\n        except ValueError:\\n            continue\\n    idx_to_test = set(cat_ids).union(cond_ids)\\n\\n    # If column name and condition given, return overlapping SampleIDs of column and\\n    # condition combinations\\n    for sid, row in imap.items():\\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\\n            key = \"_\".join([row[idx] for idx in idx_to_test])\\n            try:\\n                assert key in table.keys()\\n            except AssertionError:\\n                table[key] = DataCategory(set(), {})\\n            table[key].sids.add(sid)\\n    try:\\n        assert len(table) > 0\\n    except AssertionError:\\n        return {\"default\": DataCategory(set(imap.keys()), {})}\\n    else:\\n        return table', 'code_tokens': ['def', 'gather_categories', '(', 'imap', ',', 'header', ',', 'categories', '=', 'None', ')', ':', '# If no categories provided, return all SampleIDs', 'if', 'categories', 'is', 'None', ':', 'return', '{', '\"default\"', ':', 'DataCategory', '(', 'set', '(', 'imap', '.', 'keys', '(', ')', ')', ',', '{', '}', ')', '}', 'cat_ids', '=', '[', 'header', '.', 'index', '(', 'cat', ')', 'for', 'cat', 'in', 'categories', 'if', 'cat', 'in', 'header', 'and', '\"=\"', 'not', 'in', 'cat', ']', 'table', '=', 'OrderedDict', '(', ')', 'conditions', '=', 'defaultdict', '(', 'set', ')', 'for', 'i', ',', 'cat', 'in', 'enumerate', '(', 'categories', ')', ':', 'if', '\"=\"', 'in', 'cat', 'and', 'cat', '.', 'split', '(', '\"=\"', ')', '[', '0', ']', 'in', 'header', ':', 'cat_name', '=', 'header', '[', 'header', '.', 'index', '(', 'cat', '.', 'split', '(', '\"=\"', ')', '[', '0', ']', ')', ']', 'conditions', '[', 'cat_name', ']', '.', 'add', '(', 'cat', '.', 'split', '(', '\"=\"', ')', '[', '1', ']', ')', '# If invalid categories or conditions identified, return all SampleIDs', 'if', 'not', 'cat_ids', 'and', 'not', 'conditions', ':', 'return', '{', '\"default\"', ':', 'DataCategory', '(', 'set', '(', 'imap', '.', 'keys', '(', ')', ')', ',', '{', '}', ')', '}', '#If only category column given, return column-wise SampleIDs', 'if', 'cat_ids', 'and', 'not', 'conditions', ':', 'for', 'sid', ',', 'row', 'in', 'imap', '.', 'items', '(', ')', ':', 'cat_name', '=', '\"_\"', '.', 'join', '(', '[', 'row', '[', 'cid', ']', 'for', 'cid', 'in', 'cat_ids', ']', ')', 'if', 'cat_name', 'not', 'in', 'table', ':', 'table', '[', 'cat_name', ']', '=', 'DataCategory', '(', 'set', '(', ')', ',', '{', '}', ')', 'table', '[', 'cat_name', ']', '.', 'sids', '.', 'add', '(', 'sid', ')', 'return', 'table', '# Collect all condition names', 'cond_ids', '=', 'set', '(', ')', 'for', 'k', 'in', 'conditions', ':', 'try', ':', 'cond_ids', '.', 'add', '(', 'header', '.', 'index', '(', 'k', ')', ')', 'except', 'ValueError', ':', 'continue', 'idx_to_test', '=', 'set', '(', 'cat_ids', ')', '.', 'union', '(', 'cond_ids', ')', '# If column name and condition given, return overlapping SampleIDs of column and', '# condition combinations', 'for', 'sid', ',', 'row', 'in', 'imap', '.', 'items', '(', ')', ':', 'if', 'all', '(', '[', 'row', '[', 'header', '.', 'index', '(', 'c', ')', ']', 'in', 'conditions', '[', 'c', ']', 'for', 'c', 'in', 'conditions', ']', ')', ':', 'key', '=', '\"_\"', '.', 'join', '(', '[', 'row', '[', 'idx', ']', 'for', 'idx', 'in', 'idx_to_test', ']', ')', 'try', ':', 'assert', 'key', 'in', 'table', '.', 'keys', '(', ')', 'except', 'AssertionError', ':', 'table', '[', 'key', ']', '=', 'DataCategory', '(', 'set', '(', ')', ',', '{', '}', ')', 'table', '[', 'key', ']', '.', 'sids', '.', 'add', '(', 'sid', ')', 'try', ':', 'assert', 'len', '(', 'table', ')', '>', '0', 'except', 'AssertionError', ':', 'return', '{', '\"default\"', ':', 'DataCategory', '(', 'set', '(', 'imap', '.', 'keys', '(', ')', ')', ',', '{', '}', ')', '}', 'else', ':', 'return', 'table'], 'docstring_tokens': ['Найдите', 'указанные', 'пользователем', 'категории', 'на', 'карте', 'и', 'создайте', 'словарь', ',', 'содержащий', 'соответствующие', 'данные', 'для', 'каждого', 'типа', 'в', 'категориях', '.', 'Типы', 'нескольких', 'категорий', 'будут', 'объединены', 'таким', 'образом', ',', 'что', 'каждая', 'возможная', 'комбинация', 'будет', 'иметь', 'свою', 'собственную', 'запись', 'в', 'словаре', '.']}\n",
            "{'code': 'def parse_unifrac(unifracFN):\\n    \"\"\"\\n    Parses the unifrac results file into a dictionary\\n\\n    :type unifracFN: str\\n    :param unifracFN: The path to the unifrac results file\\n\\n    :rtype: dict\\n    :return: A dictionary with keys: \\'pcd\\' (principle coordinates data) which is a\\n             dictionary of the data keyed by sample ID, \\'eigvals\\' (eigenvalues), and\\n             \\'varexp\\' (variation explained)\\n    \"\"\"\\n    with open(unifracFN, \"rU\") as uF:\\n        first = uF.next().split(\"\\\\t\")\\n        lines = [line.strip() for line in uF]\\n\\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\\n    if first[0] == \"pc vector number\":\\n        return parse_unifrac_v1_8(unifrac, lines)\\n    elif first[0] == \"Eigvals\":\\n        return parse_unifrac_v1_9(unifrac, lines)\\n    else:\\n        raise ValueError(\"File format not supported/recognized. Please check input \"\\n                         \"unifrac file.\")', 'code_tokens': ['def', 'parse_unifrac', '(', 'unifracFN', ')', ':', 'with', 'open', '(', 'unifracFN', ',', '\"rU\"', ')', 'as', 'uF', ':', 'first', '=', 'uF', '.', 'next', '(', ')', '.', 'split', '(', '\"\\\\t\"', ')', 'lines', '=', '[', 'line', '.', 'strip', '(', ')', 'for', 'line', 'in', 'uF', ']', 'unifrac', '=', '{', '\"pcd\"', ':', 'OrderedDict', '(', ')', ',', '\"eigvals\"', ':', '[', ']', ',', '\"varexp\"', ':', '[', ']', '}', 'if', 'first', '[', '0', ']', '==', '\"pc vector number\"', ':', 'return', 'parse_unifrac_v1_8', '(', 'unifrac', ',', 'lines', ')', 'elif', 'first', '[', '0', ']', '==', '\"Eigvals\"', ':', 'return', 'parse_unifrac_v1_9', '(', 'unifrac', ',', 'lines', ')', 'else', ':', 'raise', 'ValueError', '(', '\"File format not supported/recognized. Please check input \"', '\"unifrac file.\"', ')'], 'docstring_tokens': ['Разбирает', 'файл', 'результатов', 'unifrac', 'в', 'словарь']}\n",
            "{'code': 'def parse_unifrac_v1_8(unifrac, file_data):\\n    \"\"\"\\n    Function to parse data from older version of unifrac file obtained from Qiime version\\n    1.8 and earlier.\\n\\n    :type unifrac: dict\\n    :param unifracFN: The path to the unifrac results file\\n\\n    :type file_data: list\\n    :param file_data: Unifrac data lines after stripping whitespace characters.\\n    \"\"\"\\n    for line in file_data:\\n        if line == \"\":\\n            break\\n        line = line.split(\"\\\\t\")\\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\\n\\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\\\t\")[1:]]\\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\\\t\")[1:]]\\n    return unifrac', 'code_tokens': ['def', 'parse_unifrac_v1_8', '(', 'unifrac', ',', 'file_data', ')', ':', 'for', 'line', 'in', 'file_data', ':', 'if', 'line', '==', '\"\"', ':', 'break', 'line', '=', 'line', '.', 'split', '(', '\"\\\\t\"', ')', 'unifrac', '[', '\"pcd\"', ']', '[', 'line', '[', '0', ']', ']', '=', '[', 'float', '(', 'e', ')', 'for', 'e', 'in', 'line', '[', '1', ':', ']', ']', 'unifrac', '[', '\"eigvals\"', ']', '=', '[', 'float', '(', 'entry', ')', 'for', 'entry', 'in', 'file_data', '[', '-', '2', ']', '.', 'split', '(', '\"\\\\t\"', ')', '[', '1', ':', ']', ']', 'unifrac', '[', '\"varexp\"', ']', '=', '[', 'float', '(', 'entry', ')', 'for', 'entry', 'in', 'file_data', '[', '-', '1', ']', '.', 'split', '(', '\"\\\\t\"', ')', '[', '1', ':', ']', ']', 'return', 'unifrac'], 'docstring_tokens': ['Функция', 'для', 'анализа', 'данных', 'из', 'старой', 'версии', 'файла', 'unifrac', ',', 'полученного', 'из', 'Qiime', 'версии', '1', '.', '8', 'и', 'ранее', '.']}\n",
            "{'code': 'def parse_unifrac_v1_9(unifrac, file_data):\\n    \"\"\"\\n    Function to parse data from newer version of unifrac file obtained from Qiime version\\n    1.9 and later.\\n\\n    :type unifracFN: str\\n    :param unifracFN: The path to the unifrac results file\\n\\n    :type file_data: list\\n    :param file_data: Unifrac data lines after stripping whitespace characters.\\n    \"\"\"\\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\\\t\")]\\n    unifrac[\"varexp\"] = [float(entry)*100 for entry in file_data[3].split(\"\\\\t\")]\\n\\n    for line in file_data[8:]:\\n        if line == \"\":\\n            break\\n        line = line.split(\"\\\\t\")\\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\\n    return unifrac', 'code_tokens': ['def', 'parse_unifrac_v1_9', '(', 'unifrac', ',', 'file_data', ')', ':', 'unifrac', '[', '\"eigvals\"', ']', '=', '[', 'float', '(', 'entry', ')', 'for', 'entry', 'in', 'file_data', '[', '0', ']', '.', 'split', '(', '\"\\\\t\"', ')', ']', 'unifrac', '[', '\"varexp\"', ']', '=', '[', 'float', '(', 'entry', ')', '*', '100', 'for', 'entry', 'in', 'file_data', '[', '3', ']', '.', 'split', '(', '\"\\\\t\"', ')', ']', 'for', 'line', 'in', 'file_data', '[', '8', ':', ']', ':', 'if', 'line', '==', '\"\"', ':', 'break', 'line', '=', 'line', '.', 'split', '(', '\"\\\\t\"', ')', 'unifrac', '[', '\"pcd\"', ']', '[', 'line', '[', '0', ']', ']', '=', '[', 'float', '(', 'e', ')', 'for', 'e', 'in', 'line', '[', '1', ':', ']', ']', 'return', 'unifrac'], 'docstring_tokens': ['Функция', 'для', 'анализа', 'данных', 'из', 'более', 'новой', 'версии', 'файла', 'unifrac', ',', 'полученного', 'из', 'Qiime', 'версии', '1', '.', '9', 'и', 'позже', '.']}\n",
            "{'code': 'def color_mapping(sample_map, header, group_column, color_column=None):\\n    \"\"\"\\n    Determine color-category mapping. If color_column was specified, then map the category\\n    names to color values. Otherwise, use the palettable colors to automatically generate\\n    a set of colors for the group values.\\n\\n    :type sample_map: dict\\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\\n                      sample ID (each value of the map also contains the sample ID)\\n\\n    :type header: tuple\\n    :param A tuple of header line for mapping file\\n\\n    :type group_column: str\\n    :param group_column: String denoting the column name for sample groups.\\n\\n    :type color_column: str\\n    :param color_column: String denoting the column name for sample colors.\\n\\n    :type return: dict\\n    :param return: {SampleID: Color}\\n    \"\"\"\\n    group_colors = OrderedDict()\\n    group_gather = gather_categories(sample_map, header, [group_column])\\n\\n    if color_column is not None:\\n        color_gather = gather_categories(sample_map, header, [color_column])\\n        # match sample IDs between color_gather and group_gather\\n        for group in group_gather:\\n            for color in color_gather:\\n                # allow incomplete assignment of colors, if group sids overlap at\\n                # all with the color sids, consider it a match\\n                if group_gather[group].sids.intersection(color_gather[color].sids):\\n                    group_colors[group] = color\\n    else:\\n        bcolors = itertools.cycle(Set3_12.hex_colors)\\n        for group in group_gather:\\n            group_colors[group] = bcolors.next()\\n\\n    return group_colors', 'code_tokens': ['def', 'color_mapping', '(', 'sample_map', ',', 'header', ',', 'group_column', ',', 'color_column', '=', 'None', ')', ':', 'group_colors', '=', 'OrderedDict', '(', ')', 'group_gather', '=', 'gather_categories', '(', 'sample_map', ',', 'header', ',', '[', 'group_column', ']', ')', 'if', 'color_column', 'is', 'not', 'None', ':', 'color_gather', '=', 'gather_categories', '(', 'sample_map', ',', 'header', ',', '[', 'color_column', ']', ')', '# match sample IDs between color_gather and group_gather', 'for', 'group', 'in', 'group_gather', ':', 'for', 'color', 'in', 'color_gather', ':', '# allow incomplete assignment of colors, if group sids overlap at', '# all with the color sids, consider it a match', 'if', 'group_gather', '[', 'group', ']', '.', 'sids', '.', 'intersection', '(', 'color_gather', '[', 'color', ']', '.', 'sids', ')', ':', 'group_colors', '[', 'group', ']', '=', 'color', 'else', ':', 'bcolors', '=', 'itertools', '.', 'cycle', '(', 'Set3_12', '.', 'hex_colors', ')', 'for', 'group', 'in', 'group_gather', ':', 'group_colors', '[', 'group', ']', '=', 'bcolors', '.', 'next', '(', ')', 'return', 'group_colors'], 'docstring_tokens': ['Определите', 'сопоставление', 'цветовой', 'категории', '.', 'Если', 'был', 'указан', 'color', '_', 'column', ',', 'сопоставьте', 'имена', 'категорий', 'со', 'значениями', 'цвета', '.', 'В', 'противном', 'случае', 'используйте', 'цвета', 'палитры', 'для', 'автоматического', 'создания', 'набора', 'цветов', 'для', 'групповых', 'значений', '.']}\n",
            "{'code': 'def rev_c(read):\\n    \"\"\"\\n    return reverse completment of read\\n    \"\"\"\\n    rc = []\\n    rc_nucs = {\\'A\\':\\'T\\', \\'T\\':\\'A\\', \\'G\\':\\'C\\', \\'C\\':\\'G\\', \\'N\\':\\'N\\'}\\n    for base in read:\\n        rc.extend(rc_nucs[base.upper()])\\n    return rc[::-1]', 'code_tokens': ['def', 'rev_c', '(', 'read', ')', ':', 'rc', '=', '[', ']', 'rc_nucs', '=', '{', \"'A'\", ':', \"'T'\", ',', \"'T'\", ':', \"'A'\", ',', \"'G'\", ':', \"'C'\", ',', \"'C'\", ':', \"'G'\", ',', \"'N'\", ':', \"'N'\", '}', 'for', 'base', 'in', 'read', ':', 'rc', '.', 'extend', '(', 'rc_nucs', '[', 'base', '.', 'upper', '(', ')', ']', ')', 'return', 'rc', '[', ':', ':', '-', '1', ']'], 'docstring_tokens': ['вернуть', 'обратное', 'дополнение', 'чтения']}\n",
            "{'code': 'def shuffle_genome(genome, cat, fraction = float(100), plot = True, \\\\\\n        alpha = 0.1, beta = 100000, \\\\\\n        min_length = 1000, max_length = 200000):\\n    \"\"\"\\n    randomly shuffle genome\\n    \"\"\"\\n    header = \\'>randomized_%s\\' % (genome.name)\\n    sequence = list(\\'\\'.join([i[1] for i in parse_fasta(genome)]))\\n    length = len(sequence)\\n    shuffled = []\\n    # break genome into pieces\\n    while sequence is not False:\\n        s = int(random.gammavariate(alpha, beta))\\n        if s <= min_length or s >= max_length:\\n            continue\\n        if len(sequence) < s:\\n            seq = sequence[0:]\\n        else:\\n            seq = sequence[0:s]\\n        sequence = sequence[s:]\\n#        if bool(random.getrandbits(1)) is True:\\n#            seq = rev_c(seq)\\n#            print(\\'fragment length: %s reverse complement: True\\' % (\\'{:,}\\'.format(s)), file=sys.stderr)\\n#        else:\\n#            print(\\'fragment length: %s reverse complement: False\\' % (\\'{:,}\\'.format(s)), file=sys.stderr)\\n        shuffled.append(\\'\\'.join(seq))\\n        if sequence == []:\\n            break\\n    # shuffle pieces\\n    random.shuffle(shuffled)\\n    # subset fragments\\n    if fraction == float(100):\\n        subset = shuffled\\n    else:\\n        max_pieces = int(length * fraction/100)\\n        subset, total = [], 0\\n        for fragment in shuffled:\\n            length = len(fragment)\\n            if total + length <= max_pieces:\\n                subset.append(fragment)\\n                total += length\\n            else:\\n                diff = max_pieces - total\\n                subset.append(fragment[0:diff])\\n                break\\n    # combine sequences, if requested\\n    if cat is True:\\n        yield [header, \\'\\'.join(subset)]\\n    else:\\n        for i, seq in enumerate(subset):\\n            yield [\\'%s fragment:%s\\' % (header, i), seq]', 'code_tokens': ['def', 'shuffle_genome', '(', 'genome', ',', 'cat', ',', 'fraction', '=', 'float', '(', '100', ')', ',', 'plot', '=', 'True', ',', 'alpha', '=', '0.1', ',', 'beta', '=', '100000', ',', 'min_length', '=', '1000', ',', 'max_length', '=', '200000', ')', ':', 'header', '=', \"'>randomized_%s'\", '%', '(', 'genome', '.', 'name', ')', 'sequence', '=', 'list', '(', \"''\", '.', 'join', '(', '[', 'i', '[', '1', ']', 'for', 'i', 'in', 'parse_fasta', '(', 'genome', ')', ']', ')', ')', 'length', '=', 'len', '(', 'sequence', ')', 'shuffled', '=', '[', ']', '# break genome into pieces', 'while', 'sequence', 'is', 'not', 'False', ':', 's', '=', 'int', '(', 'random', '.', 'gammavariate', '(', 'alpha', ',', 'beta', ')', ')', 'if', 's', '<=', 'min_length', 'or', 's', '>=', 'max_length', ':', 'continue', 'if', 'len', '(', 'sequence', ')', '<', 's', ':', 'seq', '=', 'sequence', '[', '0', ':', ']', 'else', ':', 'seq', '=', 'sequence', '[', '0', ':', 's', ']', 'sequence', '=', 'sequence', '[', 's', ':', ']', '#        if bool(random.getrandbits(1)) is True:', '#            seq = rev_c(seq)', \"#            print('fragment length: %s reverse complement: True' % ('{:,}'.format(s)), file=sys.stderr)\", '#        else:', \"#            print('fragment length: %s reverse complement: False' % ('{:,}'.format(s)), file=sys.stderr)\", 'shuffled', '.', 'append', '(', \"''\", '.', 'join', '(', 'seq', ')', ')', 'if', 'sequence', '==', '[', ']', ':', 'break', '# shuffle pieces', 'random', '.', 'shuffle', '(', 'shuffled', ')', '# subset fragments', 'if', 'fraction', '==', 'float', '(', '100', ')', ':', 'subset', '=', 'shuffled', 'else', ':', 'max_pieces', '=', 'int', '(', 'length', '*', 'fraction', '/', '100', ')', 'subset', ',', 'total', '=', '[', ']', ',', '0', 'for', 'fragment', 'in', 'shuffled', ':', 'length', '=', 'len', '(', 'fragment', ')', 'if', 'total', '+', 'length', '<=', 'max_pieces', ':', 'subset', '.', 'append', '(', 'fragment', ')', 'total', '+=', 'length', 'else', ':', 'diff', '=', 'max_pieces', '-', 'total', 'subset', '.', 'append', '(', 'fragment', '[', '0', ':', 'diff', ']', ')', 'break', '# combine sequences, if requested', 'if', 'cat', 'is', 'True', ':', 'yield', '[', 'header', ',', \"''\", '.', 'join', '(', 'subset', ')', ']', 'else', ':', 'for', 'i', ',', 'seq', 'in', 'enumerate', '(', 'subset', ')', ':', 'yield', '[', \"'%s fragment:%s'\", '%', '(', 'header', ',', 'i', ')', ',', 'seq', ']'], 'docstring_tokens': ['случайно', 'перетасовать', 'геном']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "s = 0\n",
        "d = []\n",
        "with open('/content/train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    s += 1\n",
        "\n",
        "    d.append(line)"
      ],
      "metadata": {
        "id": "KMjmpEPxUcEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grz0cmxmBhMt",
        "outputId": "9221cdb5-9a40-4207-a601-dc18de382b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "251820"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "c = []\n",
        "d = []\n",
        "with open('train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    example = json.loads(line)\n",
        "    c.append(example['code'])\n",
        "    comment = ' '.join(example['docstring_tokens'])\n",
        "    d.append(comment)\n",
        " "
      ],
      "metadata": {
        "id": "fhXOCyS-wlBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c[0], codes[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIccSXTLwvx5",
        "outputId": "7cafb4c7-0f77-4be5-ec68-9174d4f55594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \"\"\"\\n    level = level+\"__\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\";\")[0]',\n",
              " 'def split_phylogeny(p, level=\"s\"):\\n    \"\"\"\\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\\n\\n    :type p: str\\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\\n\\n    :type level: str\\n    :param level: The different level of identification are kingdom (k), phylum (p),\\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\\n                  not provided, the default level of identification is species.\\n\\n    :rtype: str\\n    :return: A QIIME-formatted taxonomy string up to the classification given\\n            by param level.\\n    \"\"\"\\n    level = level+\"__\"\\n    result = p.split(level)\\n    return result[0]+level+result[1].split(\";\")[0]')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " c[50000], codes[50000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b-E3g31c_0y",
        "outputId": "0ba3b029-ec43-4460-d3d8-1b6eb7029f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('def default_ubuntu_tr(mod):\\n    \"\"\"\\n    Default translation function for Ubuntu based systems\\n    \"\"\"\\n    pkg = \\'python-%s\\' % mod.lower()\\n    py2pkg = pkg\\n    py3pkg = \\'python3-%s\\' % mod.lower()\\n    return (pkg, py2pkg, py3pkg)',\n",
              " 'def default_ubuntu_tr(mod):\\n    \"\"\"\\n    Default translation function for Ubuntu based systems\\n    \"\"\"\\n    pkg = \\'python-%s\\' % mod.lower()\\n    py2pkg = pkg\\n    py3pkg = \\'python3-%s\\' % mod.lower()\\n    return (pkg, py2pkg, py3pkg)')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " c[100000], codes[100000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzMSwMpDc_2t",
        "outputId": "bd5b8744-a58f-4a90-be36-fab763f7bafa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('def free_memory(cls, exclude=None):\\n        \"\"\"Free global annotation memory.\"\"\"\\n\\n        annotations_in_memory = Annotation.__ANNOTATIONS_IN_MEMORY__\\n\\n        exclude = () if exclude is None else exclude\\n\\n        for annotation_cls in list(annotations_in_memory.keys()):\\n\\n            if issubclass(annotation_cls, exclude):\\n                continue\\n\\n            if issubclass(annotation_cls, cls):\\n                del annotations_in_memory[annotation_cls]',\n",
              " 'def free_memory(cls, exclude=None):\\n        \"\"\"Free global annotation memory.\"\"\"\\n\\n        annotations_in_memory = Annotation.__ANNOTATIONS_IN_MEMORY__\\n\\n        exclude = () if exclude is None else exclude\\n\\n        for annotation_cls in list(annotations_in_memory.keys()):\\n\\n            if issubclass(annotation_cls, exclude):\\n                continue\\n\\n            if issubclass(annotation_cls, cls):\\n                del annotations_in_memory[annotation_cls]')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " c[150000], codes[150000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Wqn7dVs1Vd3",
        "outputId": "5abdf91f-567f-4bf4-a526-4de5d3087fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('def _compile_vts(self, vts, ctx, upstream_analysis, dependency_classpath, progress_message, settings,\\n                   compiler_option_sets, zinc_file_manager, counter):\\n    \"\"\"Compiles sources for the given vts into the given output dir.\\n\\n    :param vts: VersionedTargetSet with one entry for the target.\\n    :param ctx: - A CompileContext instance for the target.\\n    :param dependency_classpath: A list of classpath entries of type ClasspathEntry for dependencies\\n\\n    May be invoked concurrently on independent target sets.\\n\\n    Postcondition: The individual targets in vts are up-to-date, as if each were\\n                   compiled individually.\\n    \"\"\"\\n    if not ctx.sources:\\n      self.context.log.warn(\\'Skipping {} compile for targets with no sources:\\\\n  {}\\'\\n                            .format(self.name(), vts.targets))\\n    else:\\n      counter_val = str(counter()).rjust(counter.format_length(), \\' \\')\\n      counter_str = \\'[{}/{}] \\'.format(counter_val, counter.size)\\n      # Do some reporting.\\n      self.context.log.info(\\n        counter_str,\\n        \\'Compiling \\',\\n        items_to_report_element(ctx.sources, \\'{} source\\'.format(self.name())),\\n        \\' in \\',\\n        items_to_report_element([t.address.reference() for t in vts.targets], \\'target\\'),\\n        \\' (\\',\\n        progress_message,\\n        \\').\\')\\n      with self.context.new_workunit(\\'compile\\', labels=[WorkUnitLabel.COMPILER]) as compile_workunit:\\n        try:\\n          directory_digest = self.compile(\\n            ctx,\\n            self._args,\\n            dependency_classpath,\\n            upstream_analysis,\\n            settings,\\n            compiler_option_sets,\\n            zinc_file_manager,\\n            self._get_plugin_map(\\'javac\\', Java.global_instance(), ctx.target),\\n            self._get_plugin_map(\\'scalac\\', ScalaPlatform.global_instance(), ctx.target),\\n          )\\n          self._capture_logs(compile_workunit, ctx.log_dir)\\n          return directory_digest\\n        except TaskError:\\n          if self.get_options().suggest_missing_deps:\\n            logs = [path\\n                    for _, name, _, path in self._find_logs(compile_workunit)\\n                    if name == self.name()]\\n            if logs:\\n              self._find_missing_deps(logs, ctx.target)\\n          raise',\n",
              " 'def _compile_vts(self, vts, ctx, upstream_analysis, dependency_classpath, progress_message, settings,\\n                   compiler_option_sets, zinc_file_manager, counter):\\n    \"\"\"Compiles sources for the given vts into the given output dir.\\n\\n    :param vts: VersionedTargetSet with one entry for the target.\\n    :param ctx: - A CompileContext instance for the target.\\n    :param dependency_classpath: A list of classpath entries of type ClasspathEntry for dependencies\\n\\n    May be invoked concurrently on independent target sets.\\n\\n    Postcondition: The individual targets in vts are up-to-date, as if each were\\n                   compiled individually.\\n    \"\"\"\\n    if not ctx.sources:\\n      self.context.log.warn(\\'Skipping {} compile for targets with no sources:\\\\n  {}\\'\\n                            .format(self.name(), vts.targets))\\n    else:\\n      counter_val = str(counter()).rjust(counter.format_length(), \\' \\')\\n      counter_str = \\'[{}/{}] \\'.format(counter_val, counter.size)\\n      # Do some reporting.\\n      self.context.log.info(\\n        counter_str,\\n        \\'Compiling \\',\\n        items_to_report_element(ctx.sources, \\'{} source\\'.format(self.name())),\\n        \\' in \\',\\n        items_to_report_element([t.address.reference() for t in vts.targets], \\'target\\'),\\n        \\' (\\',\\n        progress_message,\\n        \\').\\')\\n      with self.context.new_workunit(\\'compile\\', labels=[WorkUnitLabel.COMPILER]) as compile_workunit:\\n        try:\\n          directory_digest = self.compile(\\n            ctx,\\n            self._args,\\n            dependency_classpath,\\n            upstream_analysis,\\n            settings,\\n            compiler_option_sets,\\n            zinc_file_manager,\\n            self._get_plugin_map(\\'javac\\', Java.global_instance(), ctx.target),\\n            self._get_plugin_map(\\'scalac\\', ScalaPlatform.global_instance(), ctx.target),\\n          )\\n          self._capture_logs(compile_workunit, ctx.log_dir)\\n          return directory_digest\\n        except TaskError:\\n          if self.get_options().suggest_missing_deps:\\n            logs = [path\\n                    for _, name, _, path in self._find_logs(compile_workunit)\\n                    if name == self.name()]\\n            if logs:\\n              self._find_missing_deps(logs, ctx.target)\\n          raise')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " c[250000], codes[250000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4emuTsijQ5Wo",
        "outputId": "64c0939d-24c9-46ef-9eb3-49f79942e660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('def push_async_callback(self, callback, *args, **kwds):\\n        \"\"\"Registers an arbitrary coroutine function and arguments.\\n        Cannot suppress exceptions.\\n        \"\"\"\\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\\n\\n        # We changed the signature, so using @wraps is not appropriate, but\\n        # setting __wrapped__ may still help with introspection.\\n        _exit_wrapper.__wrapped__ = callback\\n        self._push_exit_callback(_exit_wrapper, False)\\n        return callback',\n",
              " 'def push_async_callback(self, callback, *args, **kwds):\\n        \"\"\"Registers an arbitrary coroutine function and arguments.\\n        Cannot suppress exceptions.\\n        \"\"\"\\n        _exit_wrapper = self._create_async_cb_wrapper(callback, *args, **kwds)\\n\\n        # We changed the signature, so using @wraps is not appropriate, but\\n        # setting __wrapped__ may still help with introspection.\\n        _exit_wrapper.__wrapped__ = callback\\n        self._push_exit_callback(_exit_wrapper, False)\\n        return callback')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d[0], trans_50[0]"
      ],
      "metadata": {
        "id": "gTInEMLZc_4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d[50000], trans_100[0]"
      ],
      "metadata": {
        "id": "i_UiwjJzc_66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d[100000], trans_150[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aj1TKasc_9M",
        "outputId": "807c1856-130a-493e-cd88-ebe07a8d1452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Свободная глобальная память аннотаций .',\n",
              " 'Свободная глобальная память аннотаций.')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d[150000], trans_200[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQZpg2oN1fuQ",
        "outputId": "c3fb4f8f-b51c-46bf-cb08-c28078845862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Компилирует исходники для данного vts в указанный выходной каталог .',\n",
              " 'Компилирует исходники для данного vts в указанный выходной каталог.')"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d[200000], trans_250[0].text"
      ],
      "metadata": {
        "id": "MXHLQNm7Q8NV",
        "outputId": "5d6fe850-b231-4d5a-85ef-47a08db97731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Преобразует кривую однородной матрицей преобразования tf',\n",
              " 'Преобразует кривую однородной матрицей преобразования tf')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I need to add code_tokens for 250k file"
      ],
      "metadata": {
        "id": "VVVZaWbDQR2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "codes = []\n",
        "code_tokens = []\n",
        "with open('/content/drive/MyDrive/PLBART-main/data/codeXglue/code-to-text_for_trans/python/train.jsonl', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    example = json.loads(line)\n",
        "    codes.append(example['code'])\n",
        "    code_tokens.append(example['code_tokens'])"
      ],
      "metadata": {
        "id": "ap04QECwQX_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(code_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CydusTs-QYBk",
        "outputId": "6cb71750-2058-4a8c-d5ff-b3f4e0c5ed7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_250 = []\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_50_ru', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_250.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_100_ru.txt', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_250.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_150_ru', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_250.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_200_ru', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_250.append(line.strip())\n",
        "\n",
        "with open('/content/drive/MyDrive/PLBART-main/additional_data/trans_train_250_ru', 'r', encoding='utf-8') as f:\n",
        "  for line in f:\n",
        "    trans_250.append(line.strip())\n"
      ],
      "metadata": {
        "id": "xVZvTMvHQYDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(trans_250))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTQZH4IkQYGx",
        "outputId": "668fb041-58aa-4dd4-d030-c71fe935bf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251820\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string \n",
        "\n",
        "def my_split(x):\n",
        "  temp = x.split()\n",
        "  result = []\n",
        "  for tok in temp:\n",
        "    accum = ''\n",
        "    for t in tok: \n",
        "      if t not in string.punctuation:\n",
        "        accum += t\n",
        "      else:\n",
        "        if len(accum) > 0:\n",
        "          result.append(accum)\n",
        "        accum = ''\n",
        "        result.append(t)\n",
        "    if len(accum) > 0:\n",
        "      result.append(accum)\n",
        "  return result"
      ],
      "metadata": {
        "id": "1AutTDmqQYJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/\n",
        "\n",
        "with open('train.jsonl', 'w', encoding='utf-8') as f:\n",
        "  for i in range(251820):\n",
        "    result = dict()\n",
        "    result['code'] = codes[i]\n",
        "    result['code_tokens'] = code_tokens[i]\n",
        "    result['docstring_tokens'] = my_split(trans_250[i])\n",
        "    f.write(json.dumps(result)+'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-UZXTUwQYMB",
        "outputId": "4c6a3c21-2774-4ec5-e4dd-87f2c488eac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cp /content/train.jsonl /content/drive/MyDrive/PLBART-main/additional_data/trans_train_250/"
      ],
      "metadata": {
        "id": "djITWvM5YPwD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}